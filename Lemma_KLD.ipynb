{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate information-theoretic measures of distributional\n",
    "similarity based on word frequencies in two texts\n",
    "\"\"\"\n",
    "\n",
    "import collections\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "def get_counts(word_list):\n",
    "    return collections.Counter(word_list)\n",
    "\n",
    "\n",
    "def create_prob_dist(count_dict):\n",
    "    total_ct = sum(count_dict.values())\n",
    "    p = {x: ct / total_ct for x, ct in count_dict.items()}\n",
    "    return p\n",
    "\n",
    "\n",
    "def count_smoothing(freq_dist, vocabulary, alpha=1):\n",
    "    \"\"\"Implement simple count-based probability smoothing.\n",
    "    Given a target vocabulary and a set of observed count frequencies,\n",
    "    calculate a new set of counts so that Count(x) > 0 for all words\n",
    "    in the target vocabulary.  This is achieved by adding `alpha`\n",
    "    to each observed count\n",
    "    \"\"\"\n",
    "    return {w: freq_dist.get(w, 0) + alpha for w in vocabulary}\n",
    "\n",
    "\n",
    "def entropy(p):\n",
    "    \"\"\"Calculate entropy H(p) for a probability distribution represented\n",
    "    as a mapping (dictionary) from word tokens to probabilities\n",
    "    \"\"\"\n",
    "    h = 0\n",
    "\n",
    "    # TODO -- Calculate entropy value in nats for probability distribution `p`\n",
    "    for x in p:\n",
    "        h -= p[x] * math.log(p[x])\n",
    "\n",
    "    return h\n",
    "\n",
    "\n",
    "def cross_entropy(p1, p2):\n",
    "    \"\"\"Calculate cross-entropy H(p1, p2) for two probability distributions\n",
    "    represented as a mapping (dictionary) from word tokens to\n",
    "    probabilities\n",
    "    \"\"\"\n",
    "    xh = 0\n",
    "\n",
    "    # TODO -- Calculate cross-entropy value H(p1, p2) in nats\n",
    "    for x in p1:\n",
    "        xh -= p1[x] * math.log(p2[x])\n",
    "\n",
    "    return xh\n",
    "\n",
    "\n",
    "def kl_divergence(p1, p2):\n",
    "    \"\"\"Calculate Kullback-Leibler divergence D_{KL}(p1||p2) for two\n",
    "    probability distributions represented as a mapping (dictionary)\n",
    "    from word tokens to probabilities\n",
    "    \"\"\"\n",
    "    kl = 0\n",
    "\n",
    "    # TODO -- Calculate KL divergence D_{KL}(p1||p2) in nats\n",
    "    kl = cross_entropy(p1, p2) - entropy(p1)\n",
    "\n",
    "    return kl\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cc-tokens.txt') as f:\n",
    "    cc_tokens = f.read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('biology-tokens.txt') as f:\n",
    "    biology_tokens = f.read().lower().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'spite', 'of', 'the', 'morphological', 'and', 'developmental', 'differences', 'between', 'vertebrate']\n"
     ]
    }
   ],
   "source": [
    "print(biology_tokens[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3953291406940274\n",
      "1.3917604586536854\n"
     ]
    }
   ],
   "source": [
    "# biology\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(biology_tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4892076798817788\n",
      "1.370622024644197\n"
     ]
    }
   ],
   "source": [
    "with open('chemistry-tokens.txt') as f:\n",
    "    biology_tokens = f.read().lower().splitlines()\n",
    "    \n",
    "# chemistry\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(biology_tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.100465612965392\n",
      "1.026490990467484\n"
     ]
    }
   ],
   "source": [
    "with open('computer-tokens.txt') as f:\n",
    "    tokens = f.read().lower().splitlines()\n",
    "    \n",
    "# computer science\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7843515970190342\n",
      "0.6954866376363178\n"
     ]
    }
   ],
   "source": [
    "with open('economics-tokens.txt') as f:\n",
    "    tokens = f.read().lower().splitlines()\n",
    "    \n",
    "# economics\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9704102935975527\n",
      "0.8744780093122486\n"
     ]
    }
   ],
   "source": [
    "with open('engineering-tokens.txt') as f:\n",
    "    tokens = f.read().lower().splitlines()\n",
    "    \n",
    "# engineering\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8798484125993156\n",
      "1.009452987475873\n"
     ]
    }
   ],
   "source": [
    "with open('philosophy-tokens.txt') as f:\n",
    "    tokens = f.read().lower().splitlines()\n",
    "    \n",
    "# philosophy\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8602824444201698\n",
      "0.8758825158417558\n"
     ]
    }
   ],
   "source": [
    "with open('psychology-tokens.txt') as f:\n",
    "    tokens = f.read().lower().splitlines()\n",
    "    \n",
    "# psychology\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5554565950380557\n",
      "0.5205863947668457\n"
     ]
    }
   ],
   "source": [
    "with open('sociology-tokens.txt') as f:\n",
    "    tokens = f.read().lower().splitlines()\n",
    "    \n",
    "# sociology\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7645831018494826\n",
      "1.0654439636740385\n"
     ]
    }
   ],
   "source": [
    "with open('art-tokens.txt') as f:\n",
    "    tokens = f.read().lower().splitlines()\n",
    "    \n",
    "# sociology\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5620148386710095\n",
      "1.39688622264058\n"
     ]
    }
   ],
   "source": [
    "with open('physics-tokens.txt') as f:\n",
    "    tokens = f.read().lower().splitlines()\n",
    "    \n",
    "# sociology\n",
    "ct_a = get_counts(cc_tokens)\n",
    "# print(ct_a)\n",
    "ct_b = get_counts(tokens)\n",
    "# print(ct_b)\n",
    "\n",
    "vocab = set(ct_a.keys()) | set(ct_b.keys())\n",
    "ct_a = count_smoothing(ct_a, vocab)\n",
    "# print(ct_a)\n",
    "ct_b = count_smoothing(ct_b, vocab)\n",
    "\n",
    "p_a = create_prob_dist(ct_a)\n",
    "p_b = create_prob_dist(ct_b)\n",
    "\n",
    "\n",
    "kl_ab = kl_divergence(p_a, p_b)\n",
    "print(kl_ab)\n",
    "    \n",
    "kl_ba = kl_divergence(p_b, p_a)\n",
    "print(kl_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
